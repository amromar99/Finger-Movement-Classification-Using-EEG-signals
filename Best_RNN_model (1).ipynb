{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Motor Imagery EEG Signal Analysis for Finger Movement: An Approach to Brain-Computer Interfaces**\n",
        "\n",
        "---\n",
        "`In this study, we aimed to classify finger movements using brain signals captured through EEG. We focused on five distinct classes of finger movements: Thumb, Index, Middle, Ring, and Pinky. Utilizing RNN for feature extraction from EEG data, we reduced the complexity by using only 4 channels out of the original 64. This reduction was achieved before applying Continuous Wavelet Transform (CWT) analysis with the Morlet mother wavelet and a scale of 10\n",
        "`\n",
        "\n"
      ],
      "metadata": {
        "id": "8n-1O1yLV3o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eng/Amr Mostafa Omar\n",
        "# Nile University ,Cairo, Egypt\n",
        "# Data 20/6/2024"
      ],
      "metadata": {
        "id": "NU1cLZo9gJ5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9-1dg8_6oUV",
        "outputId": "d4116fdf-8389-4e3a-95f9-6111fbcd6ad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0ZCfsBimGoF"
      },
      "outputs": [],
      "source": [
        "pip install PyWavelets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import welch, butter, filtfilt\n",
        "from scipy.signal import butter, filtfilt\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pywt\n",
        "import psutil\n",
        "import os"
      ],
      "metadata": {
        "id": "LMcpw_cCaXLs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBiSdeIpJQeU"
      },
      "source": [
        "##**Step 1: Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wBv6pvFJQ9J"
      },
      "outputs": [],
      "source": [
        "# Function to load data from a .mat file\n",
        "def load_data_from_mat(mat_file_path):\n",
        "    mat_data = scipy.io.loadmat(mat_file_path)\n",
        "    data = mat_data['Data'][0, 0]\n",
        "    X = data['trials']  # EEG data (256, 64, 900)\n",
        "    y = data['Labels'].flatten()  # Target labels (900,)\n",
        "    Fs = data['Fs'][0, 0]  # Sampling frequency\n",
        "    Channels = [ch[0] for ch in data['Channels'][0]]  # EEG channels\n",
        "    return X, y, Fs, Channels\n",
        "\n",
        "# List of file paths\n",
        "mat_file_paths = [\n",
        "    '/content/drive/MyDrive/Folder 1/Power_Spectrum_trials_subject_1.mat',\n",
        "    '/content/drive/MyDrive/Folder 1/Power_Spectrum_trials_subject_2.mat',\n",
        "    '/content/drive/MyDrive/Folder 1/Power_Spectrum_trials_subject_3.mat',\n",
        "    '/content/drive/MyDrive/Folder 1/Power_Spectrum_trials_subject_4.mat',\n",
        "    '/content/drive/MyDrive/Folder 1/Power_Spectrum_trials_subject_5.mat'\n",
        "]\n",
        "\n",
        "\n",
        "# Initialize lists to hold all data\n",
        "X_all = []\n",
        "Y_all = []\n",
        "\n",
        "# Load all data\n",
        "for mat_file_path in mat_file_paths:\n",
        "    X, y, Fs, Channels = load_data_from_mat(mat_file_path)\n",
        "    X_all.append(X)\n",
        "    Y_all.append(y)\n",
        "\n",
        "# Convert lists to arrays\n",
        "X = np.concatenate(X_all, axis=2)  # Concatenate along the trials axis\n",
        "y = np.concatenate(Y_all)  # Concatenate labels\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"Combined EEG Data Shape (Trials):\", X.shape)\n",
        "print(\"Combined Labels Shape:\", y.shape)\n",
        "print(\"Sampling Frequency:\", Fs)\n",
        "print(\"Channels:\", Channels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SQdR7zWJZI_"
      },
      "source": [
        "##**Step 2: Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0B_xZn5kJSa4"
      },
      "outputs": [],
      "source": [
        "# Define band-pass filter parameters\n",
        "lowcut = 8.0\n",
        "highcut = 30.0\n",
        "order = 5\n",
        "\n",
        "# Function to apply band-pass filter\n",
        "def bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    filtered_data = filtfilt(b, a, data, axis=0)\n",
        "    return filtered_data\n",
        "\n",
        "# Apply band-pass filter to all trials and channels\n",
        "filtered_data = np.zeros_like(X)\n",
        "for trial in range(X.shape[2]):\n",
        "    for channel in range(X.shape[1]):\n",
        "        filtered_data[:, channel, trial] = bandpass_filter(X[:, channel, trial], lowcut, highcut, Fs, order=order)\n",
        "\n",
        "# Common Average Referencing (CAR)\n",
        "mean_signal = np.mean(filtered_data, axis=1, keepdims=True)\n",
        "filtered_data -= mean_signal\n",
        "\n",
        "# Define the list of channels to keep\n",
        "selected_channels = ['T7',  'TP7','PO3',  'PO7']\n",
        "\n",
        "\n",
        "# Get indices of selected channels\n",
        "channel_indices = [Channels.index(ch) for ch in selected_channels]\n",
        "\n",
        "# Select only the specified channels\n",
        "filtered_data_selected = filtered_data[:, channel_indices, :]\n",
        "\n",
        "# Print shape to verify\n",
        "print(\"Filtered and CAR Processed Data Shape (Selected Channels):\", filtered_data_selected.shape)\n",
        "\n",
        "# Reshape 3D data to 2D\n",
        "reshaped_data = filtered_data_selected.reshape(-1, filtered_data_selected.shape[1])\n",
        "\n",
        "# Apply RobustScaler\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(reshaped_data)\n",
        "\n",
        "# Reshape back to 3D if necessary\n",
        "scaled_data_3d = scaled_data.reshape(filtered_data_selected.shape)\n",
        "\n",
        "print(\"Scaled Data Shape (Selected Channels):\", scaled_data_3d.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgrWhdYyPMm3"
      },
      "source": [
        "##**Step 3 :Split Data into Classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJwXoqyAgCYp"
      },
      "outputs": [],
      "source": [
        "# Initialize arrays for different classes (Thumb, Index, Middle, Ring, Pinky)\n",
        "X_thumb, Y_thumb = None, None\n",
        "X_index, Y_index = None, None\n",
        "X_middle, Y_middle = None, None\n",
        "X_ring, Y_ring = None, None\n",
        "X_pinky, Y_pinky = None, None\n",
        "\n",
        "# Function to split and assign data\n",
        "def assign_data_to_class(class_label, scaled_data_3d, y, percentage=0.25):\n",
        "    # Select the target trials for the current class\n",
        "    target_trials = scaled_data_3d[:, :, y == class_label]\n",
        "    num_target_trials = target_trials.shape[2]\n",
        "\n",
        "    # Determine the number of non-target trials to select from each other class\n",
        "    num_non_target_trials_per_class = int(percentage * num_target_trials)\n",
        "\n",
        "    # Select the non-target trials from other classes (non-randomly, using the first 25%)\n",
        "    non_target_trials = []\n",
        "    for other_class_label in range(1, 6):\n",
        "        if other_class_label != class_label:\n",
        "            other_class_trials = scaled_data_3d[:, :, y == other_class_label]\n",
        "            non_target_trials.append(other_class_trials[:, :, :num_non_target_trials_per_class])\n",
        "    non_target_trials = np.concatenate(non_target_trials, axis=2)\n",
        "\n",
        "    # Combine target and non-target trials\n",
        "    combined_trials = np.concatenate((target_trials, non_target_trials), axis=2)\n",
        "    combined_labels = np.array([1] * num_target_trials + [0] * non_target_trials.shape[2])  # 1 for target, 0 for non-target\n",
        "\n",
        "    return combined_trials, combined_labels\n",
        "\n",
        "# Assign data to each class\n",
        "X_thumb, Y_thumb = assign_data_to_class(1, scaled_data_3d, y)\n",
        "X_index, Y_index = assign_data_to_class(2, scaled_data_3d, y)\n",
        "X_middle, Y_middle = assign_data_to_class(3, scaled_data_3d, y)\n",
        "X_ring, Y_ring = assign_data_to_class(4, scaled_data_3d, y)\n",
        "X_pinky, Y_pinky = assign_data_to_class(5, scaled_data_3d, y)\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"Thumb Data Shape (Trials):\", X_thumb.shape, \"Labels Shape:\", Y_thumb.shape)\n",
        "print(\"Index Data Shape (Trials):\", X_index.shape, \"Labels Shape:\", Y_index.shape)\n",
        "print(\"Middle Data Shape (Trials):\", X_middle.shape, \"Labels Shape:\", Y_middle.shape)\n",
        "print(\"Ring Data Shape (Trials):\", X_ring.shape, \"Labels Shape:\", Y_ring.shape)\n",
        "print(\"Pinky Data Shape (Trials):\", X_pinky.shape, \"Labels Shape:\", Y_pinky.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWiM353AlHby"
      },
      "source": [
        "##**Step 4: Split Data into Training and Testing Sets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQkHOHpOiYJC"
      },
      "outputs": [],
      "source": [
        "# Function to split data into train and test sets\n",
        "def split_data(trials, labels):\n",
        "    # Split into train and test sets (80:20 ratio)\n",
        "    train_trials, test_trials, train_labels, test_labels = train_test_split(\n",
        "        trials.transpose(2, 0, 1), labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "\n",
        "    # Transpose back to (256, 64, num_trials) shape\n",
        "    train_trials = train_trials.transpose(1, 2, 0)\n",
        "    test_trials = test_trials.transpose(1, 2, 0)\n",
        "\n",
        "    return train_trials, test_trials, train_labels, test_labels\n",
        "\n",
        "# Split each part into train and test sets\n",
        "X_train_thumb, X_test_thumb, Y_train_thumb, Y_test_thumb = split_data(X_thumb, Y_thumb)\n",
        "X_train_index, X_test_index, Y_train_index, Y_test_index = split_data(X_index, Y_index)\n",
        "X_train_middle, X_test_middle, Y_train_middle, Y_test_middle = split_data(X_middle, Y_middle)\n",
        "X_train_ring, X_test_ring, Y_train_ring, Y_test_ring = split_data(X_ring, Y_ring)\n",
        "X_train_pinky, X_test_pinky, Y_train_pinky, Y_test_pinky = split_data(X_pinky, Y_pinky)\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"Thumb Train Data Shape (Trials):\", X_train_thumb.shape, \"Train Labels Shape:\", Y_train_thumb.shape)\n",
        "print(\"Thumb Test Data Shape (Trials):\", X_test_thumb.shape, \"Test Labels Shape:\", Y_test_thumb.shape)\n",
        "\n",
        "print(\"Index Train Data Shape (Trials):\", X_train_index.shape, \"Train Labels Shape:\", Y_train_index.shape)\n",
        "print(\"Index Test Data Shape (Trials):\", X_test_index.shape, \"Test Labels Shape:\", Y_test_index.shape)\n",
        "\n",
        "print(\"Middle Train Data Shape (Trials):\", X_train_middle.shape, \"Train Labels Shape:\", Y_train_middle.shape)\n",
        "print(\"Middle Test Data Shape (Trials):\", X_test_middle.shape, \"Test Labels Shape:\", Y_test_middle.shape)\n",
        "\n",
        "print(\"Ring Train Data Shape (Trials):\", X_train_ring.shape, \"Train Labels Shape:\", Y_train_ring.shape)\n",
        "print(\"Ring Test Data Shape (Trials):\", X_test_ring.shape, \"Test Labels Shape:\", Y_test_ring.shape)\n",
        "\n",
        "print(\"Pinky Train Data Shape (Trials):\", X_train_pinky.shape, \"Train Labels Shape:\", Y_train_pinky.shape)\n",
        "print(\"Pinky Test Data Shape (Trials):\", X_test_pinky.shape, \"Test Labels Shape:\", Y_test_pinky.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mB82IPHlNt6"
      },
      "source": [
        "#**Step 5: Apply Continuous Wavelet Transform (CWT)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D46iPATvlQEa"
      },
      "outputs": [],
      "source": [
        "def apply_cwt(data):\n",
        "    scales = np.arange(1, 101)  #  scales from 1 to 100\n",
        "    cwt_coeffs = []\n",
        "    for trial in data.transpose(2, 1, 0):  # Transpose to (trials, channels, samples)\n",
        "        trial_coeffs = []\n",
        "        for channel in trial:\n",
        "            coeffs, freqs = pywt.cwt(channel, scales, 'morl')  # wavelet 'morl'\n",
        "            trial_coeffs.append(coeffs)\n",
        "        cwt_coeffs.append(trial_coeffs)\n",
        "    cwt_coeffs = np.array(cwt_coeffs)\n",
        "    # Transpose to get shape (trials, samples, coefficients, channels)\n",
        "    cwt_coeffs = cwt_coeffs.transpose(0, 2, 3, 1)\n",
        "    return cwt_coeffs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 6 :Train and Evaluate RNN model**"
      ],
      "metadata": {
        "id": "02NxsiAivOtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory management functions\n",
        "def print_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(f\"Memory usage: {process.memory_info().rss / 1024 ** 2:.2f} MB\")\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    print_memory_usage()\n",
        "\n",
        "class ClearMemory(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        clear_memory()\n",
        "\n",
        "# Transformer block for numerical data\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Define datasets and labels (as per your current setup)\n",
        "datasets = {\n",
        "    'thumb': (X_train_thumb, X_test_thumb, Y_train_thumb, Y_test_thumb),\n",
        "    'index': (X_train_index, X_test_index, Y_train_index, Y_test_index),\n",
        "    'middle': (X_train_middle, X_test_middle, Y_train_middle, Y_test_middle),\n",
        "    'ring': (X_train_ring, X_test_ring, Y_train_ring, Y_test_ring),\n",
        "    'pinky': (X_train_pinky, X_test_pinky, Y_train_pinky, Y_test_pinky),\n",
        "}\n",
        "\n",
        "results = {}"
      ],
      "metadata": {
        "id": "8JJqvvYN8Gx_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tctdBQqS6Miz"
      },
      "outputs": [],
      "source": [
        "# Loop over each dataset\n",
        "for finger, (X_train, X_test, Y_train, Y_test) in datasets.items():\n",
        "\n",
        "    # Step 3: Organize Data into RNN-Ready Format using CWT\n",
        "    X_train_cwt = apply_cwt(X_train)\n",
        "    X_test_cwt = apply_cwt(X_test)\n",
        "\n",
        "    # Concatenate CWT coefficients across channels\n",
        "    X_train_concat = np.concatenate(X_train_cwt, axis=-1)\n",
        "    X_test_concat = np.concatenate(X_test_cwt, axis=-1)\n",
        "\n",
        "    # Clear memory after concatenating CWT coefficients\n",
        "    del X_train_cwt, X_test_cwt\n",
        "    gc.collect()\n",
        "\n",
        "    # Reshape data to match RNN input shape (timesteps, features)\n",
        "    X_train_concat = X_train_concat.reshape(-1, 100, 256 * 4)\n",
        "    X_test_concat = X_test_concat.reshape(-1, 100, 256 * 4)\n",
        "\n",
        "    # Build RNN model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(128, input_shape=(100, 256 * 4), return_sequences=True))\n",
        "    model.add(LSTM(64, return_sequences=False))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer=Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # Train model with custom callback to clear memory after each epoch\n",
        "    history = model.fit(X_train_concat, Y_train, epochs=15, batch_size=32, validation_data=(X_test_concat, Y_test), callbacks=[ClearMemory()])\n",
        "\n",
        "    # Evaluate model\n",
        "    loss, accuracy = model.evaluate(X_test_concat, Y_test)\n",
        "    results[finger] = accuracy\n",
        "    print(f\"Accuracy for {finger}: {accuracy:.4f}\")\n",
        "\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title(f'{finger.capitalize()} Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title(f'{finger.capitalize()} Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Predict the values from the test set\n",
        "    Y_pred = (model.predict(X_test_concat) > 0.5).astype(\"int32\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(Y_test, Y_pred)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Target', 'Target'], yticklabels=['Not Target', 'Target'])\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title(f'{finger.capitalize()} Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Classification report\n",
        "    print(f'{finger.capitalize()} Classification Report:')\n",
        "    print(classification_report(Y_test, Y_pred, target_names=['Not Target', 'Target']))\n",
        "\n",
        "# Print final accuracies\n",
        "print(\"Final accuracies:\")\n",
        "for finger, accuracy in results.items():\n",
        "    print(f\"{finger}: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMeKveW4xDIQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}